+++
title: NDC 2017 <로보리콜> 포스트모템 정리
tags: [ndc]
slug: ndc-2017-robo-recall
author: if1live
date: 2017-05-11
url: /posts/ndc-2017-robo-recall
+++

2017년 4월 26일에 NDC 2017을 갔다왔다.
잊어버리기전에 기억나는 내용을 정리해본다.
나와 관련있는 부분 아니면 기록도 안해놨다.
그래서 전체 내용을 알아보고 싶으면 다른 자료를 보는게 좋을 것이다.


## 세션 소개

* 제목 : <로보리콜> 포스트모템
* 발표분야 : 가상현실
* 발표내용의난이도 : 기본적인 사전지식 필요
* 수강권장대상 : VR 개발자
* 학생참관여부 : 학생 참관 가능
* 공개수준 : L1-Public Open

발표자 소개 : 신광섭 - 에픽게임즈코리아

현재 Developer Relations Lead이자 프로그래머로서 근무중이며,
이전에는 소프트맥스에서 언리얼 엔진 2, 3를 이용해서
마그나카르타 PS2/PSP와 Xbox 360 버전을 개발했습니다.

세션소개

이 세션에서는 '로보리콜'을 개발하며 중점을 둔,
몰입감 있는 VR 경험을 선사하기 위해서 어떤 게임플레이적인 요소들에 대한 고려를 했는지와
초당 90프레임으로 처리해야하는 VR 환경에서 '로보리콜'의 높은 그래픽 퀄리티와 게임 플레이를 구현하기 위해서,
렌더링과 프레임워크 부분에서 이루어진 기술적 개선점과 최적화 방법에 대해서 다루어봅니다.

## 내용

* 개요
    * 개발 기간 : 12개월
    * 개발팀 규모 : 15명 + 외주 (외주에 대해서는 뒤에서 자세히 다룬다)
    * 최소사양 : i5 + GTX 970 (오큘러스 최소사양)

* 렌더링 스펙
    * 배경
        * 900 draw call
        * 1.8M polygon
    * 배경+캐릭터
        * 1200 draw call
        * 2.2M polygon
    * Actor
        * 1600 tick function
        * actor count < 250
        * tick function은 유니티의 update()와 비슷한거라고 하더라

* 세션 타임
    * VR 게임하는 사람들은 보통 20분 정도 플레이한다고 카더라
    * 집중해서 게임하는 시간은 6-7분 정도
    * 스테이지당 15분정도로 설계

* 시선 방향은 어떻게 결정되었는가?
    * 텔레포트하기 전에 결정하는 방향은 유저가 바라볼 방향이 아니다
    * 텔레포트하기 전게 결정하는 방향은 센서의 방향이다
    * 오큘러스 터치는 기본적으로 센서 2개를 쓴다
    * 센서 2개로는 정면밖에 인식 못한다
    * 플레이하면서 앞의 방향이 틀어질수 있다.
    * 하지만 유저는 앞의 방향이 틀어졌는지 알수없다 (헤드셋을 뒤집어쓰고 있으니까)
    * 텔레포트에서 사용하는 방향을 센서가 있는 방향으로 설정해서 플레이어가 계속 센서를 보도록 유도

* 잘 보이는곳에 퀄리티 몰아주기
    * 로봇의 몸통은 눈높이니까 항상 보인다. 다리를 웬만해서는 볼일없다.
    * -> 항상 보이는 부위에 디테일 몰빵
    * -> 모든 부위를 잘 만들기에는 비용이 비싸니까

* 최종 캐릭터 모델
    * 30K vertex
    * 캐릭터당 텍스쳐 2-3장
        * 예: 로봇 (upper body + lower body + head/neck)
        * 성능 문제상 여러장의 텍스쳐 겹쳐서 효과내는게 어렵다
        * normal map, specular map,,, 등을 전부 쓰진 않은듯

* 플레이어
    * 팔은 그럴싸하게 만들기 어렵다. 그래서 삭제. 손만 떠다닌다.
    * 만약 팔을 그린다면 IK를 써야되는데 잘못 만들면 없는것보다 몰입감이 적다.
        * 팔꿈치를 알 방법이 없어서 그럴싸한 팔을 만들기 어렵다.
        * (의견) 내가 그렇게 구현된거 해봤거든.
    * 로봇 손인지 인간 손인지 애매하게 적당히 모델링

* 외주
    * 주로 배경을 외주
    * 사진 기반으로 외주 맡기는것만 가능한 것만
    * 그래야 결과물을 통제하기 쉬우니까
        * 뜬구름 잡는걸 외주맡기면 뜬구름 잡는 결과물이 나오니까
    * 예: 사진에 있는 건물과 똑같은거 만들어주세요. 근데 사진과 달리 에어콘 실외기는 빼주세요.
    * 파트를 적당히 쪼개도록 요청
        * 창문, 창틀, 벽,,, 을 쪼갬.
        * 조합해서 가로로 창문이 4개인 집, 6개인 집을 만들 수 있다.
        * 재사용하기 좋으니까
        * 대신 나중에 최적화해야된다

* 렌더링
    * deferred rendering : 일반적으로 90~100% 해상도로 렌더링
    * forward rendering : 일반적으로 70~100% 해상도로 렌더링후 MSAA (x2 ~ x4) 적용
    * 로보리콜은 forward rendering 사용
    * 성능상의 문제 때문에
    * 디퍼드 렌더링을 쓰면 90fps를 맞출수 없었다.
        * 90fps를 맞추려면 한 프레임을 11.1ms 안에 렌더링해야된다.
        * 로보리콜의 경우 디퍼드 렌더러는 렌더링만 11ms를 넘었음.
        * 포워드 렌더러로 바꾸고 8~9ms안에 렌더링 할 수 있었다
    * overdraw 주의
        * MSAA를 쓸때 overdraw는 성능에 영향을 준다
        * LOD를 적용해서 문제를 해결

## 후기

나도 [VR게임][toyclash]는 하나 만들어봤다.
나는 타켓 플랫폼이 모바일VR인 기어VR인데 로보리콜은 PC VR인 오큘러스이다.
그래서 여러가지 생각을 하면서 들었다.

예를 들어 기어VR에서의 draw call 한계는 대략 100이다.
근데 양쪽눈을 각각 그려야하니 실제로는 50 draw call이 한계다.
(내가 개발할때는 Single-Pass Stereo rendering가 없었다. 나중에 생겼고 그나마 PC에서만 지원했다)
로보리콜은 1200 draw call을 찍어다고카더라.
세상에 드로우콜부터 20배 넘게 차이난다니!

게다가 게임에서 사용한 폴리곤의 수는 단위가 M이래! M이라니! 백만폴리곤이래!
나는 K였는데! (기어VR에서는 50k – 100k polygons per frame이 권장이다)

이외에도 여러 가지가 있지만 그건 나중에 생각나면 써봐야지.

## 외부 링크

슬라이드를 공개 안하는거같더라.
그마나 볼만한 링크만 정리해보자

* [에픽게임스 '로보리콜' 개발 비법 공개](http://www.vrn.co.kr/news/articleView.html?idxno=61)

[toyclash]: https://www.oculus.com/experiences/gear-vr/1407846952568081/
